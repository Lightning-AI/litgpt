
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../pretrain/">
      
      
        <link rel="next" href="../python-api/">
      
      
        
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.7.2">
    
    
      
        <title>Pretrain TinyLlama - LitGPT Tutorials</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.484c7ddc.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../assets/mkdocs_pagetree_plugin.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#pretrain-tinyllama" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="LitGPT Tutorials" class="md-header__button md-logo" aria-label="LitGPT Tutorials" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            LitGPT Tutorials
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Pretrain TinyLlama
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="LitGPT Tutorials" class="md-nav__button md-logo" aria-label="LitGPT Tutorials" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    LitGPT Tutorials
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Home
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../0_to_litgpt/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Zero to LitGPT: Getting Started with Pretraining, Finetuning, and Using LLMs
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../convert_hf_checkpoint/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Converting Hugging Face Transformers to LitGPT weights
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../convert_lit_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Convert lit models
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../deploy/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Serve and Deploy LLMs
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../download_model_weights/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Download Model Weights with LitGPT
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../evaluation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    LLM Evaluation
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../finetune/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Finetuning
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../finetune_adapter/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Finetuning with Adapter
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../finetune_full/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Finetuning the whole model
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../finetune_lora/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Finetuning with LoRA / QLoRA
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../inference/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Inference
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../oom/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Oom
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../prepare_dataset/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Preparing Datasets
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../pretrain/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Pretrain LLMs with LitGPT
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    
  
    Pretrain TinyLlama
  

    
  </span>
  
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    
  
    Pretrain TinyLlama
  

    
  </span>
  
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#whats-tinyllama" class="md-nav__link">
    <span class="md-ellipsis">
      
        What's TinyLlama?
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#download-datasets" class="md-nav__link">
    <span class="md-ellipsis">
      
        Download datasets
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#prepare-the-datasets-for-training" class="md-nav__link">
    <span class="md-ellipsis">
      
        Prepare the datasets for training
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pretraining" class="md-nav__link">
    <span class="md-ellipsis">
      
        Pretraining
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#resume-training" class="md-nav__link">
    <span class="md-ellipsis">
      
        Resume training
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#export-checkpoints" class="md-nav__link">
    <span class="md-ellipsis">
      
        Export checkpoints
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#project-templates" class="md-nav__link">
    <span class="md-ellipsis">
      
        Project templates
      
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../python-api/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    LitGPT Python API
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../quantize/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Quantize the model
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../resource-tables/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Resource Tables
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_20" >
        
          
          <label class="md-nav__link" for="__nav_20" id="__nav_20_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Developer docs
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_20_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_20">
            <span class="md-nav__icon md-icon"></span>
            
  
    Developer docs
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../developer-docs/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Index
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../developer-docs/adding-models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Adding New Models
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../developer-docs/python-api/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    LitGPT High-level Python API
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_21" >
        
          
          <label class="md-nav__link" for="__nav_21" id="__nav_21_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Examples
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_21_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_21">
            <span class="md-nav__icon md-icon"></span>
            
  
    Examples
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_21_1" >
        
          
          <label class="md-nav__link" for="__nav_21_1" id="__nav_21_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Ptl trainer
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_21_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_21_1">
            <span class="md-nav__icon md-icon"></span>
            
  
    Ptl trainer
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/ptl-trainer/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Index
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#whats-tinyllama" class="md-nav__link">
    <span class="md-ellipsis">
      
        What's TinyLlama?
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#download-datasets" class="md-nav__link">
    <span class="md-ellipsis">
      
        Download datasets
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#prepare-the-datasets-for-training" class="md-nav__link">
    <span class="md-ellipsis">
      
        Prepare the datasets for training
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pretraining" class="md-nav__link">
    <span class="md-ellipsis">
      
        Pretraining
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#resume-training" class="md-nav__link">
    <span class="md-ellipsis">
      
        Resume training
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#export-checkpoints" class="md-nav__link">
    <span class="md-ellipsis">
      
        Export checkpoints
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#project-templates" class="md-nav__link">
    <span class="md-ellipsis">
      
        Project templates
      
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="pretrain-tinyllama">Pretrain TinyLlama</h1>
<p>This tutorial will walk you through pretraining <a href="https://github.com/jzhang38/TinyLlama/">TinyLlama</a>.</p>
<blockquote>
<p>[!TIP]
To get started with zero setup, clone the <a href="https://lightning.ai/lightning-ai/studios/llm-pretrain-tinyllama-1-1b">TinyLlama studio on Lightning AI</a>.</p>
</blockquote>
<p>&nbsp;</p>
<h2 id="whats-tinyllama">What's TinyLlama?</h2>
<p><a href="https://github.com/jzhang38/TinyLlama/">TinyLlama</a> is architecturally the same as Meta AI's LLama 2, but only has 1.1B parameters and is instead trained on multiple epochs on a mix of <a href="https://huggingface.co/datasets/cerebras/SlimPajama-627B">SlimPajama</a> and <a href="https://huggingface.co/datasets/bigcode/starcoderdata">Starcoder</a> datasets.</p>
<p>Here is a quick fact sheet:</p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Parameters</td>
<td>1.1B</td>
</tr>
<tr>
<td>Model Size</td>
<td>Layers: 22, Heads: 32, Query Groups: 4, Embedding Size: 2048, Intermediate Size: 5632</td>
</tr>
<tr>
<td>Sequence Length</td>
<td>2048</td>
</tr>
<tr>
<td>Learning Rate</td>
<td>4e-4</td>
</tr>
<tr>
<td>Learning Rate Schedule</td>
<td>Cosine with 2000 warmup steps</td>
</tr>
<tr>
<td>Training Data</td>
<td><a href="https://huggingface.co/datasets/cerebras/slimpajama-627b">SlimPajama</a> (893 GB), <a href="https://huggingface.co/datasets/bigcode/starcoderdata">Starcoder</a> (290 GB)</td>
</tr>
<tr>
<td>Combined Dataset Size</td>
<td>Around 950B tokens</td>
</tr>
<tr>
<td>Total Tokens During Training</td>
<td>3 trillion (3 epochs)</td>
</tr>
<tr>
<td>Time to complete training</td>
<td>~ 4 weeks with 64 A100 GPUs</td>
</tr>
<tr>
<td>Model FLOPs Utilization (MFU)</td>
<td>52%</td>
</tr>
</tbody>
</table>
<p>(this table was sourced from the author's <a href="https://github.com/jzhang38/TinyLlama/">README</a>)</p>
<p>&nbsp;</p>
<h2 id="download-datasets">Download datasets</h2>
<p>You can download the data using git lfs:</p>
<pre><code class="language-bash"># Make sure you have git-lfs installed (https://git-lfs.com):
sudo apt install git-lfs
</code></pre>
<pre><code class="language-bash">git clone https://huggingface.co/datasets/cerebras/slimpajama-627b data/slimpajama-raw
git clone https://huggingface.co/datasets/bigcode/starcoderdata data/starcoderdata-raw
</code></pre>
<p>Around 1.2 TB of disk space is required to store both datasets.</p>
<p>&nbsp;</p>
<h2 id="prepare-the-datasets-for-training">Prepare the datasets for training</h2>
<p>In order to start pretraining litgpt on it, you need to read, tokenize, and write the data in binary chunks. This will leverage the <code>litdata</code> optimization pipeline and streaming dataset.</p>
<p>First, install additional dependencies for preprocessing:</p>
<pre><code class="language-bash">pip install '.[all]'
</code></pre>
<p>You will need to have the tokenizer config available:</p>
<pre><code class="language-bash">litgpt download meta-llama/Llama-2-7b-hf \
   --access_token your_hf_token \
   --tokenizer_only true
</code></pre>
<p>Then, run the preprocessing script for each dataset and split.
You will require <strong>1.1 TB</strong> of disk space for Starcoder and <strong>2.5</strong> TB of space for the SlimPajama dataset.</p>
<p><strong>Starcoder:</strong></p>
<pre><code class="language-bash">python litgpt/data/prepare_starcoder.py \
  --input_dir data/starcoderdata-raw \
  --output_dir data/starcoder \
  --tokenizer_path checkpoints/meta-llama/Llama-2-7b-hf
</code></pre>
<p><strong>SlimPajama:</strong></p>
<pre><code class="language-bash">python litgpt/data/prepare_slimpajama.py \
  --input_dir data/slimpajama-raw/validation \
  --output_dir data/slimpajama/val \
  --tokenizer_path checkpoints/meta-llama/Llama-2-7b-hf

python litgpt/data/prepare_slimpajama.py \
  --input_dir data/slimpajama-raw/test \
  --output_dir data/slimpajama/test \
  --tokenizer_path checkpoints/meta-llama/Llama-2-7b-hf

python litgpt/data/prepare_slimpajama.py \
  --input_dir data/slimpajama-raw/train \
  --output_dir data/slimpajama/train \
  --tokenizer_path checkpoints/meta-llama/Llama-2-7b-hf
</code></pre>
<p>If you want to run on a small slice of the datasets first, pass the flag <code>--fast_dev_run=true</code> to the commands above.
In the above we are assuming that you will be using the same tokenizer as used in LlaMA/TinyLlama, but any trained <a href="https://github.com/google/sentencepiece">SentencePiece</a> tokenizer with a 32000 vocabulary size will do here.</p>
<p>&nbsp;</p>
<h2 id="pretraining">Pretraining</h2>
<p>Running the pretraining script with its default settings requires at least 8 A100 GPUs.</p>
<pre><code class="language-bash">litgpt pretrain --config config_hub/pretrain/tinyllama.yaml
</code></pre>
<p>&nbsp;</p>
<blockquote>
<p>[!TIP]
Use the <code>litgpt pretrain --data.help TinyLlama</code> command to list additional dataset options.
&nbsp;</p>
</blockquote>
<p>The script will save checkpoints periodically to the folder <code>out/</code>.
By default, the <code>pretrain</code> script will pretrain the model with FSDP in
<code>bfloat16</code> mixed precision and gradient accumulation.</p>
<p>Note that <code>pretrain</code> is not actually a model-specific training script, so feel free <a href="../config_hub">try other configurations</a>
or change the model type and size by passing a different string to the model name argument, for example:</p>
<pre><code class="language-shell">litgpt pretrain Gemma-2b
</code></pre>
<p>The currently supported model names can be listed by executing <code>litgpt pretrain</code> without any additional arguments.</p>
<p>Keep in mind that training with a single machine will take weeks. To speed up the process, you'll need access to a cluster.
Once you're in a cluster, you can follow <a href="https://lightning.ai/docs/fabric/stable/fundamentals/launch.html#launch-on-a-cluster">these instructions</a>
to launch the script across machines:</p>
<ul>
<li><a href="https://lightning.ai/docs/fabric/stable/guide/multi_node/cloud.html">Lightning AI</a></li>
<li><a href="https://lightning.ai/docs/fabric/stable/guide/multi_node/slurm.html">SLURM cluster</a></li>
<li><a href="https://lightning.ai/docs/fabric/stable/guide/multi_node/barebones.html">Barebones cluster</a></li>
<li><a href="https://lightning.ai/docs/fabric/stable/guide/multi_node/other.html">MPI</a></li>
</ul>
<p>The script exposes several hyperparameters you can tweak through the command line.</p>
<p>For instance, <code>--train.micro_batch_size</code> should be adjusted so the process will use the available
GPU memory. For more tips to avoid out-of-memory issues, please also see the more detailed
<a href="../oom/">Dealing with out-of-memory (OOM) errors</a> guide.</p>
<p>Last, logging is kept minimal in the script, but for long-running experiments we recommend switching to a proper experiment tracker.
LitGPT supports multiple experiment trackers including:</p>
<ul>
<li><strong>TensorBoard</strong> (default): Local visualization with TensorBoard</li>
<li><strong>CSV Logger</strong>: Simple local logging to CSV files</li>
<li><strong>WandB</strong>: Cloud-based experiment tracking with Weights &amp; Biases</li>
<li><strong>MLflow</strong>: MLflow experiment tracking</li>
<li><strong><a href="https://github.com/Lightning-AI/LitLogger">LitLogger</a></strong>: Lightning.ai's native experiment tracking (set <code>--logger_name=litlogger</code>)</li>
</ul>
<p>As an example, we included WandB (set <code>--logger_name=wandb</code>) to show how you can integrate any experiment tracking framework.
For reference, <a href="https://api.wandb.ai/links/awaelchli/y7pzdpwy">here are the loss curves for our reproduction</a>.</p>
<p>&nbsp;</p>
<h2 id="resume-training">Resume training</h2>
<p>The checkpoints saved during pretraining contain all the information to resume if needed.
Simply rerun the script with the <code>--resume</code> argument added:</p>
<pre><code class="language-bash">litgpt pretrain tiny-llama\
  --config config_hub/pretrain/tinyllama.yaml \
  --resume out/pretrain/tiny-llama/step-00060500
</code></pre>
<p><strong>Important:</strong> Each checkpoint is a directory. Point to the directory, not the 'lit_model.pth' file inside of it.</p>
<p>&nbsp;</p>
<blockquote>
<p>[!TIP]
Use the <code>litgpt pretrain --data.help TinyLlama</code> command to list additional dataset options.
&nbsp;</p>
</blockquote>
<p>&nbsp;</p>
<h2 id="export-checkpoints">Export checkpoints</h2>
<p>After training is completed, you can convert the checkpoint to a format that can be loaded for evaluation, inference, finetuning etc.</p>
<pre><code class="language-bash">litgpt convert_pretrained_checkpoint out/pretrain/tiny-llama/step-00060500 \
  --output_dir checkpoints/tiny-llama/final
</code></pre>
<p>After conversion, the output folder will contain these files:</p>
<pre><code>checkpoints/tiny-llama/final
├── model_config.yaml
├── lit_model.pth
├── tokenizer_config.json
├── tokenizer.json
└── tokenizer.model
</code></pre>
<p>You can then use this checkpoint folder to run <a href="../evaluation/">evaluation</a>, <a href="../inference/">inference</a>, <a href="../finetune_lora/">finetuning</a> or <a href="../convert_lit_models/">process the checkpoint further</a>.</p>
<p>&nbsp;</p>
<h2 id="project-templates">Project templates</h2>
<p>The following <a href="https://lightning.ai/lightning-ai/studios">Lightning Studio</a> templates provide LitGPT pretraining projects in reproducible environments with multi-GPU and multi-node support:
&nbsp;</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td><p align="left"><a href="https://lightning.ai/lightning-ai/studios/prepare-the-tinyllama-1t-token-dataset">Prepare the TinyLlama 1T token dataset</a> <br> <a href="https://lightning.ai/lightning-ai/studios/prepare-the-tinyllama-1t-token-dataset"><img src="https://pl-public-data.s3.amazonaws.com/assets_litgpt/readme/3.webp" width="300"></p></a></td>
<td><a href="https://lightning.ai/lightning-ai/studios/pretrain-llms-tinyllama-1-1b">Pretrain LLMs - TinyLlama 1.1B</a> <br> <p align="left"><a href="https://lightning.ai/lightning-ai/studios/pretrain-llms-tinyllama-1-1b"><img src="https://pl-public-data.s3.amazonaws.com/assets_litgpt/readme/4.webp" width="300"></p></a></td>
</tr>
<tr>
<td><a href="https://lightning.ai/lightning-ai/studios/continued-pretraining-with-tinyllama-1-1b">Continued Pretraining with TinyLlama 1.1B</a> <br> <p align="left"><a href="https://lightning.ai/lightning-ai/studios/continued-pretraining-with-tinyllama-1-1b"><img src="https://pl-public-data.s3.amazonaws.com/assets_litgpt/readme/1.webp" width="300"></p></a></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      
      <script id="__config" type="application/json">{"annotate": null, "base": "..", "features": [], "search": "../assets/javascripts/workers/search.2c215733.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../assets/javascripts/bundle.79ae519e.min.js"></script>
      
        <script src="../assets/mkdocs_pagetree_plugin.js"></script>
      
    
  </body>
</html>