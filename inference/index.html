
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../finetune_lora/">
      
      
        <link rel="next" href="../oom/">
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.21">
    
    
      
        <title>Inference - LitGPT Tutorials</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.2a3383ac.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../assets/mkdocs_pagetree_plugin.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#inference" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="LitGPT Tutorials" class="md-header__button md-logo" aria-label="LitGPT Tutorials" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            LitGPT Tutorials
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Inference
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="LitGPT Tutorials" class="md-nav__button md-logo" aria-label="LitGPT Tutorials" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    LitGPT Tutorials
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../0_to_litgpt/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Zero to LitGPT: Getting Started with Pretraining, Finetuning, and Using LLMs
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../convert_hf_checkpoint/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Converting Hugging Face Transformers to LitGPT weights
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../convert_lit_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Convert lit models
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../deploy/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Serve and Deploy LLMs
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../download_model_weights/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Download Model Weights with LitGPT
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../evaluation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    LLM Evaluation
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../finetune/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Finetuning
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../finetune_adapter/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Finetuning with Adapter
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../finetune_full/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Finetuning the whole model
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../finetune_lora/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Finetuning with LoRA / QLoRA
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    Inference
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    Inference
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#run-interactively" class="md-nav__link">
    <span class="md-ellipsis">
      Run interactively
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#run-a-large-model-on-one-smaller-device" class="md-nav__link">
    <span class="md-ellipsis">
      Run a large model on one smaller device
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#run-a-large-model-on-multiple-smaller-devices" class="md-nav__link">
    <span class="md-ellipsis">
      Run a large model on multiple smaller devices
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Run a large model on multiple smaller devices">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#litgpt-generate_sequentially" class="md-nav__link">
    <span class="md-ellipsis">
      litgpt generate_sequentially
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#litgpt-generate_tp" class="md-nav__link">
    <span class="md-ellipsis">
      litgpt generate_tp
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../oom/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Oom
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../prepare_dataset/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Preparing Datasets
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../pretrain/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Pretrain LLMs with LitGPT
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../pretrain_tinyllama/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Pretrain TinyLlama
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../python-api/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    LitGPT Python API
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../quantize/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Quantize the model
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../resource-tables/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Resource Tables
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_20" >
        
          
          <label class="md-nav__link" for="__nav_20" id="__nav_20_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Developer docs
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_20_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_20">
            <span class="md-nav__icon md-icon"></span>
            Developer docs
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../developer-docs/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Index
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../developer-docs/adding-models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Adding New Models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../developer-docs/python-api/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    LitGPT High-level Python API
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_21" >
        
          
          <label class="md-nav__link" for="__nav_21" id="__nav_21_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Examples
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_21_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_21">
            <span class="md-nav__icon md-icon"></span>
            Examples
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_21_1" >
        
          
          <label class="md-nav__link" for="__nav_21_1" id="__nav_21_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Ptl trainer
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_21_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_21_1">
            <span class="md-nav__icon md-icon"></span>
            Ptl trainer
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/ptl-trainer/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Index
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#run-interactively" class="md-nav__link">
    <span class="md-ellipsis">
      Run interactively
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#run-a-large-model-on-one-smaller-device" class="md-nav__link">
    <span class="md-ellipsis">
      Run a large model on one smaller device
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#run-a-large-model-on-multiple-smaller-devices" class="md-nav__link">
    <span class="md-ellipsis">
      Run a large model on multiple smaller devices
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Run a large model on multiple smaller devices">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#litgpt-generate_sequentially" class="md-nav__link">
    <span class="md-ellipsis">
      litgpt generate_sequentially
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#litgpt-generate_tp" class="md-nav__link">
    <span class="md-ellipsis">
      litgpt generate_tp
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="inference">Inference</h1>
<p>We demonstrate how to run inference (next token prediction) with the GPT base model in the <a href="../litgpt/generate/base.py"><code>litgpt generate</code></a> command:</p>
<pre><code class="language-bash">litgpt generate stabilityai/stablelm-base-alpha-3b \
  --prompt &quot;Hello, my name is&quot;
</code></pre>
<p>Output:</p>
<pre><code class="language-text">Hello, my name is Levi Durrer, I'm an Austrian journalist - Chairman of the Press Blair Party, with 37 years in the Press Blair International, and two years in the Spectre of Austerity for the other. I'm crossing my fingers that you will feel
</code></pre>
<p>The script assumes you have downloaded and converted the weights as described <a href="../download_model_weights/">here</a>.</p>
<p>This will run the 3B pre-trained model and require ~7 GB of GPU memory using the <code>bfloat16</code> datatype.</p>
<h2 id="run-interactively">Run interactively</h2>
<p>You can also chat with the model interactively:</p>
<pre><code class="language-bash">litgpt chat stabilityai/stablelm-tuned-alpha-3b
</code></pre>
<p>This script can work with any checkpoint. For the best chat-like experience, we recommend using it with a checkpoints
fine-tuned for chatting such as <code>stabilityai/stablelm-tuned-alpha-3b</code> or <code>togethercomputer/RedPajama-INCITE-Chat-3B-v1</code>.</p>
<blockquote>
<p>[!TIP]
Use <code>--multiline true</code> to work with inputs that span multiple lines.</p>
</blockquote>
<h2 id="run-a-large-model-on-one-smaller-device">Run a large model on one smaller device</h2>
<p>Check out our <a href="../quantize/">quantization tutorial</a>.</p>
<h2 id="run-a-large-model-on-multiple-smaller-devices">Run a large model on multiple smaller devices</h2>
<p>We offer two scripts to leverage multiple devices for inference.</p>
<h3 id="litgpt-generate_sequentially"><a href="../litgpt/generate/sequentially.py"><code>litgpt generate_sequentially</code></a></h3>
<p>Allows you to run models that wouldn't fit in a single card by partitioning the transformer blocks across all your devices and running them sequentially.</p>
<p>For instance, <code>meta-llama/Llama-2-70b-chat-hf</code> would require ~140 GB of GPU memory to load on a single device, plus the memory for activations.
With 80 transformer layers, we could partition them across 8, 5, 4, or 2 devices.</p>
<pre><code class="language-shell">litgpt generate_sequentially meta-llama/Llama-2-70b-chat-hf \
  --max_new_tokens 256 \
  --num_samples 2
</code></pre>
<p>Using A100 40GB GPUs, we need to use at least 4. You can control the number of devices by setting the <code>CUDA_VISIBLE_DEVICES=</code> environment variable.</p>
<table>
<thead>
<tr>
<th>Devices</th>
<th>Max GPU RAM</th>
<th>Token/sec</th>
</tr>
</thead>
<tbody>
<tr>
<td>2</td>
<td>OOM</td>
<td>-</td>
</tr>
<tr>
<td>4</td>
<td>35.64 GB</td>
<td>7.55</td>
</tr>
<tr>
<td>5</td>
<td>28.72 GB</td>
<td>7.49</td>
</tr>
<tr>
<td>8</td>
<td>18.35 GB</td>
<td>7.47</td>
</tr>
</tbody>
</table>
<p>Note that the memory usage will also depend on the <code>max_new_tokens</code> value used.</p>
<p>The script also supports quantization, using 4-bit precision, we can now use 2 GPUs</p>
<pre><code class="language-shell">litgpt generate_sequentially meta-llama/Llama-2-70b-chat-hf \
  --max_new_tokens 256 \
  --num_samples 2 \
  --quantize bnb.nf4-dq
</code></pre>
<table>
<thead>
<tr>
<th>Devices</th>
<th>Max GPU RAM</th>
<th>Token/sec</th>
</tr>
</thead>
<tbody>
<tr>
<td>2</td>
<td>20.00 GB</td>
<td>8.63</td>
</tr>
<tr>
<td>4</td>
<td>10.80 GB</td>
<td>8.23</td>
</tr>
<tr>
<td>5</td>
<td>8.96 GB</td>
<td>8.10</td>
</tr>
<tr>
<td>8</td>
<td>6.23 GB</td>
<td>8.18</td>
</tr>
</tbody>
</table>
<p>Smaller devices can also be used to run inference with this technique.</p>
<h3 id="litgpt-generate_tp"><a href="../litgpt/generate/tp.py"><code>litgpt generate_tp</code></a></h3>
<p>Uses tensor parallelism (TP) to run models that wouldn't fit in a single card by sharding the MLP and Attention QKV linear layers across all your devices.</p>
<p>For instance, <code>meta-llama/Llama-2-70b-chat-hf</code> would require ~140 GB of GPU memory to load on a single device, plus the memory for activations.
The requirement is that the intermediate size (for the MLP) and the QKV size (for attention) is divisible by the number of devices.
With an intermediate size of 28672, we can use 2, 4, 7, or 8 devices. With a QKV size of 10240 we can use 2, 4, 5, or 8 devices.
Since the script is configured to shard both, the intersection is used: we can only use 2, 4, or 8 devices.</p>
<pre><code class="language-shell">litgpt generate_tp meta-llama/Llama-2-70b-chat-hf \
  --max_new_tokens 256 \
  --num_samples 2
</code></pre>
<p>Using A100 40GB GPUs, we need to use at least 4. You can control the number of devices by setting the <code>CUDA_VISIBLE_DEVICES=</code> environment variable.</p>
<table>
<thead>
<tr>
<th>Devices</th>
<th>Max GPU RAM</th>
<th>Token/sec</th>
</tr>
</thead>
<tbody>
<tr>
<td>2</td>
<td>OOM</td>
<td>-</td>
</tr>
<tr>
<td>4</td>
<td>35.46 GB</td>
<td>9.33</td>
</tr>
<tr>
<td>8</td>
<td>18.19 GB</td>
<td>8.61</td>
</tr>
</tbody>
</table>
<p>Note that the memory usage will also depend on the <code>max_new_tokens</code> value used.</p>
<p>The script also supports quantization, using 4-bit precision, we can now use 2 GPUs</p>
<pre><code class="language-shell">litgpt generate_tp meta-llama/Llama-2-70b-chat-hf \
  --max_new_tokens 256 \
  --num_samples 2 \
  --quantize bnb.nf4-dq
</code></pre>
<table>
<thead>
<tr>
<th>Devices</th>
<th>Max GPU RAM</th>
<th>Token/sec</th>
</tr>
</thead>
<tbody>
<tr>
<td>2</td>
<td>19.79 GB</td>
<td>6.72</td>
</tr>
<tr>
<td>4</td>
<td>10.73 GB</td>
<td>6.48</td>
</tr>
<tr>
<td>8</td>
<td>6.15 GB</td>
<td>6.20</td>
</tr>
</tbody>
</table>
<p>Smaller devices can also be used to run inference with this technique.</p>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "..", "features": [], "search": "../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../assets/javascripts/bundle.f55a23d4.min.js"></script>
      
        <script src="../assets/mkdocs_pagetree_plugin.js"></script>
      
    
  </body>
</html>