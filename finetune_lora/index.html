
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../finetune_full/">
      
      
        <link rel="next" href="../inference/">
      
      
        
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.7.2">
    
    
      
        <title>Finetuning with LoRA / QLoRA - LitGPT Tutorials</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.484c7ddc.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../assets/mkdocs_pagetree_plugin.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#finetuning-with-lora-qlora" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="LitGPT Tutorials" class="md-header__button md-logo" aria-label="LitGPT Tutorials" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            LitGPT Tutorials
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Finetuning with LoRA / QLoRA
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="LitGPT Tutorials" class="md-nav__button md-logo" aria-label="LitGPT Tutorials" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    LitGPT Tutorials
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Home
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../0_to_litgpt/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Zero to LitGPT: Getting Started with Pretraining, Finetuning, and Using LLMs
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../convert_hf_checkpoint/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Converting Hugging Face Transformers to LitGPT weights
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../convert_lit_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Convert lit models
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../deploy/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Serve and Deploy LLMs
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../download_model_weights/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Download Model Weights with LitGPT
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../evaluation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    LLM Evaluation
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../finetune/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Finetuning
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../finetune_adapter/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Finetuning with Adapter
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../finetune_full/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Finetuning the whole model
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    
  
    Finetuning with LoRA / QLoRA
  

    
  </span>
  
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    
  
    Finetuning with LoRA / QLoRA
  

    
  </span>
  
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#preparation" class="md-nav__link">
    <span class="md-ellipsis">
      
        Preparation
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#running-the-finetuning" class="md-nav__link">
    <span class="md-ellipsis">
      
        Running the Finetuning
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#test-the-model" class="md-nav__link">
    <span class="md-ellipsis">
      
        Test the Model
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#tune-on-your-dataset" class="md-nav__link">
    <span class="md-ellipsis">
      
        Tune on Your Dataset
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#merging-lora-weights-optional" class="md-nav__link">
    <span class="md-ellipsis">
      
        Merging LoRA Weights (Optional)
      
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../inference/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Inference
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../oom/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Oom
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../prepare_dataset/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Preparing Datasets
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../pretrain/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Pretrain LLMs with LitGPT
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../pretrain_tinyllama/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Pretrain TinyLlama
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../python-api/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    LitGPT Python API
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../quantize/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Quantize the model
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../resource-tables/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Resource Tables
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_20" >
        
          
          <label class="md-nav__link" for="__nav_20" id="__nav_20_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Developer docs
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_20_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_20">
            <span class="md-nav__icon md-icon"></span>
            
  
    Developer docs
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../developer-docs/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Index
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../developer-docs/adding-models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Adding New Models
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../developer-docs/python-api/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    LitGPT High-level Python API
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_21" >
        
          
          <label class="md-nav__link" for="__nav_21" id="__nav_21_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Examples
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_21_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_21">
            <span class="md-nav__icon md-icon"></span>
            
  
    Examples
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_21_1" >
        
          
          <label class="md-nav__link" for="__nav_21_1" id="__nav_21_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Ptl trainer
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_21_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_21_1">
            <span class="md-nav__icon md-icon"></span>
            
  
    Ptl trainer
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/ptl-trainer/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Index
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#preparation" class="md-nav__link">
    <span class="md-ellipsis">
      
        Preparation
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#running-the-finetuning" class="md-nav__link">
    <span class="md-ellipsis">
      
        Running the Finetuning
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#test-the-model" class="md-nav__link">
    <span class="md-ellipsis">
      
        Test the Model
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#tune-on-your-dataset" class="md-nav__link">
    <span class="md-ellipsis">
      
        Tune on Your Dataset
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#merging-lora-weights-optional" class="md-nav__link">
    <span class="md-ellipsis">
      
        Merging LoRA Weights (Optional)
      
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="finetuning-with-lora-qlora">Finetuning with LoRA / QLoRA</h1>
<p><a href="https://arxiv.org/abs/2106.09685">Low-rank adaption (LoRA)</a> is a technique to approximate the update to the linear layers in a LLM with a low-rank matrix factorization. This significantly reduces the number of trainable parameters and speeds up training with little impact on the final performance of the model.
We demonstrate this method by instruction-finetuning LitGPT StableLM 3B on the <a href="https://github.com/tatsu-lab/stanford_alpaca">Alpaca</a> dataset on a <strong>single RTX 3090 (24GB) GPU</strong> with CUDA 11.8.</p>
<p>&nbsp;</p>
<h2 id="preparation">Preparation</h2>
<p>The steps here only need to be done once:</p>
<ol>
<li>Follow the instructions in the <a href="../README.md">README</a> to install the dependencies.</li>
<li>Download and convert the weights and save them in the <code>./checkpoints</code> folder.
   Weights can be downloaded following the instructions in the <a href="../download_model_weights/">download_model_weights</a> documentation:</li>
</ol>
<p>LitGPT provides common datasets for finetuning, such as Alpaca, LIMA, Dolly, and more.
You can optionally <a href="#tune-on-your-dataset">prepare your own dataset</a>.
For more information about dataset preparation, also see the <a href="../prepare_dataset/">prepare_dataset.md</a> tutorial.</p>
<p>&nbsp;</p>
<h2 id="running-the-finetuning">Running the Finetuning</h2>
<pre><code class="language-bash">litgpt finetune_lora stabilityai/stablelm-base-alpha-3b \
  --data Alpaca
</code></pre>
<p>The finetuning requires at least one GPU with ~24 GB memory (RTX 3090).</p>
<p>This script will save checkpoints periodically to the folder <code>out/</code>.</p>
<blockquote>
<p>[!NOTE]
LoRA can be applied to not only <code>query</code>, <code>key</code> or <code>value</code> matrices, but also to <code>projection</code>, <code>mlp</code> and classification <code>head</code>.
According to <a href="https://arxiv.org/abs/2305.14314">QLoRA</a> paper (section 4): "LoRA on all linear transformer block layers are required to match full finetuning performance".
By default LoRA is applied only to the <code>query</code> and <code>value</code> matrices. In order to apply LoRA to other weight matrices - change the arguments to <code>litgpt/finetune/lora.py</code> accordingly.</p>
</blockquote>
<p>Optionally, finetuning using 4-bit quantization (as in QLoRA) can be enabled via the <code>--quantize</code> flag, for example using the 4-bit NormalFloat data type:</p>
<pre><code class="language-bash">litgpt finetune_lora stabilityai/stablelm-base-alpha-3b \
  --quantize &quot;bnb.nf4&quot;
</code></pre>
<p>and optionally with double-quantization:</p>
<pre><code class="language-bash">litgpt finetune_lora stabilityai/stablelm-base-alpha-3b \
  --quantize &quot;bnb.nf4-dq&quot;
</code></pre>
<p>The table below lists a comparison with different settings on a StableLM 3B model finetuned with LoRA on Alpaca for 1,000 iterations using a microbatch size of 1:</p>
<table>
<thead>
<tr>
<th>Settings</th>
<th>Training Memory</th>
<th>Training Time</th>
<th>Inference Memory</th>
</tr>
</thead>
<tbody>
<tr>
<td>Default (bf16-mixed)</td>
<td>26.92 GB</td>
<td>1.34 min</td>
<td>21.43 GB</td>
</tr>
<tr>
<td>--precision bf16-true</td>
<td>9.69 GB</td>
<td>1.24 min</td>
<td>7.30 GB</td>
</tr>
<tr>
<td>--precision bf16-true --quantize bnb.nf4</td>
<td>6.35 GB</td>
<td>1.82 min</td>
<td>3.20 GB</td>
</tr>
<tr>
<td>--precision bf16-true --quantize bnb.nf4-dq</td>
<td>6.19 GB</td>
<td>1.87 min</td>
<td>3.04 GB</td>
</tr>
</tbody>
</table>
<p>The advantages of QLoRA-style quantization are more pronounced in larger models, such as Llama 2 7B. The table below summarizes the results for Llama 2 7B on Alpaca for 1,000 iterations using a microbatch size of 1:</p>
<table>
<thead>
<tr>
<th>Settings</th>
<th>Training Memory</th>
<th>Training Time</th>
<th>Inference Memory</th>
</tr>
</thead>
<tbody>
<tr>
<td>Default (bf16-mixed)</td>
<td>OutOfMemoryError</td>
<td>N/A</td>
<td>40.21 GB</td>
</tr>
<tr>
<td>--precision bf16-true</td>
<td>21.30 GB</td>
<td>2.36 min</td>
<td>13.52 GB</td>
</tr>
<tr>
<td>--precision bf16-true --quantize bnb.nf4</td>
<td>14.14 GB</td>
<td>3.68 min</td>
<td>4.57 GB</td>
</tr>
<tr>
<td>--precision bf16-true --quantize bnb.nf4-dq</td>
<td>13.84 GB</td>
<td>3.83 min</td>
<td>4.26 GB</td>
</tr>
</tbody>
</table>
<p>For additional benchmarks and resource requirements, please see the <a href="../resource-tables/">Resource Tables</a>.</p>
<p>&nbsp;</p>
<h2 id="test-the-model">Test the Model</h2>
<p>You can test the finetuned model with your own instructions by running:</p>
<pre><code class="language-bash">litgpt generate &quot;out/lora/final&quot; \
  --prompt &quot;Recommend a movie to watch on the weekend.&quot;
</code></pre>
<p>Output:</p>
<pre><code class="language-text">I would recommend the movie The Martian (2015). It is a sci-fi movie starring Matt Damon that follows the story of...
</code></pre>
<p>If your GPU supports <code>bfloat16</code>, you can additionally pass <code>--precision "bf16-true"</code> to bring the memory consumption down to ~7.6 GB for StableLM-3B (versus ~15.2  GB for <code>--precision "32-full"</code>). In addition, you may use quantization methods, for example <code>--precision "bf16-true" --quantize "bnb.nf4"</code> brings the memory consumption further down to ~4.4 GB for StableLM-3B.</p>
<p>&nbsp;</p>
<h2 id="tune-on-your-dataset">Tune on Your Dataset</h2>
<p>You can easily train on your own instruction dataset saved in JSON format.</p>
<ol>
<li>
<p>Create a JSON file in which each row holds one instruction-response pair.
   A row has an entry for 'instruction' and 'output', and optionally 'input'. Note that currently, the 'input' field is only used in the Alpaca chat template. If you are using the Alpaca template, 'input' can be the empty string if the instruction doesn't require a context.
   Below is an example json file:</p>
<p><code>text
[
    {
        "instruction": "Arrange the given numbers in ascending order.",
        "input": "2, 4, 0, 8, 3", // Optional: only used in Alpaca chat template
        "output": "0, 2, 3, 4, 8"
    },
    ...
]</code></p>
</li>
<li>
<p>Run <code>litgpt finetune_lora</code> by passing in the location of your data (and optionally other parameters):</p>
<p><code>bash
litgpt finetune_lora checkpoints/stabilityai/stablelm-base-alpha-3b \
    --data JSON \
    --data.json_path data/mydata.json \
    --out_dir out_dir/mydata-finetuned</code></p>
</li>
<li>
<p>Test and use the finetuned model:</p>
<p><code>bash
litgpt chat out_dir/mydata-finetuned/final</code></p>
</li>
</ol>
<p>or</p>
<pre><code>```bash
litgpt serve out_dir/mydata-finetuned/final
```
</code></pre>
<p>&nbsp;</p>
<h2 id="merging-lora-weights-optional">Merging LoRA Weights (Optional)</h2>
<p>Finetuning a model with LoRA generates a <code>lit_model.pth.lora</code> file.
This file exclusively contains the LoRA weights, which are much smaller than the original model checkpoint to conserve storage space.</p>
<blockquote>
<p>[!NOTE]
LitGPT will automatically merge the checkpoint for you if you use it in any of the inference commands, such as <code>litgpt generate</code> or <code>litgpt chat</code>.
Manual merging is only necessary if you want to use the checkpoint outside LitGPT.</p>
</blockquote>
<p>If desired, there is the option to merge these LoRA weights manually into the original model's checkpoint, which creates a full <code>lit_model.pth</code> checkpoint.
The advantage of this merging process is to streamline inference operations, as it eliminates the need to dynamically incorporate the LoRA weights during runtime, which can improve inference speed.</p>
<p>For example, after finetuning produced a checkpoint folder <code>out/lora/step-002000</code>, merge it as follows:</p>
<pre><code class="language-bash">litgpt merge_lora &quot;out/lora/step-002000&quot;
</code></pre>
<p>The command above creates a full <code>lit_model.pth</code> checkpoint file.</p>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      
      <script id="__config" type="application/json">{"annotate": null, "base": "..", "features": [], "search": "../assets/javascripts/workers/search.2c215733.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../assets/javascripts/bundle.79ae519e.min.js"></script>
      
        <script src="../assets/mkdocs_pagetree_plugin.js"></script>
      
    
  </body>
</html>